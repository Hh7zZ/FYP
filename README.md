<div align="center">

# Generalizing Zero-Shot Robotic Grasping through One-Shot Open-Vocabulary Affordance Learning

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

</div>

## üìñ Abstract

As robotic applications expand from structured industrial environments to unstructured domestic service domains, robots are required not only to recognize object categories but also to comprehend object affordance to facilitate effective interaction. Addressing the challenges inherent in traditional robotic grasping methods‚Äîspecifically their heavy reliance on massive annotated datasets and confinement to closed-set vocabularies‚Äîthis paper proposes a **"One-Shot Open Affordance Learning"** framework.

This framework integrates the **DINOv3** model, employed for extracting fine-grained visual features, with the **CLIP** model enhanced by **Context Optimization (CoOp)**. Through the introduction of a **CLS-Guided Transformer Decoder**, the system achieves a precise mapping from abstract semantic instructions to pixel-level affordance regions. Unlike methods requiring millions of training iterations, the proposed approach achieves lightweight training using **only one annotated example** in each base category and demonstrates robust zero-shot inference capabilities.

## üìä Results & Visualization

The figure below demonstrates the model's capability to generalize affordance learning to both seen and unseen categories.

<div align="center">
  <img src="assets/affordance_visualization.png" alt="Affordance Prediction Visualization" width="100%">
  <br>
  <em>Figure 1: Qualitative results showing input images (top row) and generated affordance heatmaps (bottom row). The model accurately identifies regions for actions like "Cut", "Hold", "Open" and "Carry" across various objects.</em>
</div>

<br>

## üõ†Ô∏è Installation

```bash
# Clone the repository
git clone [https://github.com/your-username/your-repo-name.git](https://github.com/your-username/your-repo-name.git)
cd your-repo-name

# Install dependencies
pip install -r requirements.txt

## üöÄ Usage

### Training
To train the model using the One-Shot setting:

```bash
python train.py
```

### Inference
To infer the affordance area in the objects:

```bash
python demo.py \
  --img "path_to_image" \
  --model_file "path_to_trained_weights"
```
